import pickle
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from catboost import CatBoostClassifier

from backend.src import constants

class MlModels:
  def __init__(self):
    self.root_models_path = constants.root_pretrained_models_path

  def test_using_saved_models(self, model_name, X_test):
    # Load the stored model
    with open(self.root_models_path + "\\" + model_name + ".pkl" , 'rb') as f:
      model = pickle.load(f)
      f.close()

    # Reformat the feature vector for evaluation
    test = np.array(X_test)
    dTest = np.array([test])
    
    # Make prediction using model
    y_pred = model.predict(dTest)
    
    # Generate the appropriate result.
    if y_pred[0] == 0:
      return "Benign"
    return "Malware"

  # Split the dataset into specified ratio:
  def split_dataset(self, dataset_with_labels, test_size_in_split=0.2):
    X = dataset_with_labels.iloc[:, :-1]
    y = dataset_with_labels.iloc[:,-1]
    return train_test_split(X, y, test_size=test_size_in_split, random_state=42)
  
  #Used for scaling the continuous data
  def scaling(self, X_train, X_test, ScalerType=MinMaxScaler):
    scaler = ScalerType()
    Xtr = scaler.fit_transform(X_train)
    Xte = scaler.transform(X_test)
    return (Xtr, Xte)

  # Training functions for different ML models
  # SVM
  def runSVM(self, X_train, y_train,  hyperparams):
    svm=SVC(C=10, probability=True, kernel='rbf', gamma='1')
    svm.fit(X_train, y_train)
    return svm
  
  # Random Forest
  def runRandomForest(self, X_train, y_train,  hyperparams):
    forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    forest.fit(X_train, y_train)
    return forest
  
  # KNN
  def runKNN(self, X_train, y_train,  hyperparams):
    knn = KNeighborsClassifier(n_neighbors=5, p=1)
    knn.fit(X_train, y_train)
    return knn

  # Naive Bayes
  def runLogReg(self, X_train, y_train,  hyperparams):
    lgr = LogisticRegression(penalty='l2', C=10, solver='sag')
    lgr.fit(X_train, y_train)
    return lgr

  # CatBoost
  def runCatBoost(self, X_train, y_train,  hyperparams):
    cat = CatBoostClassifier(iterations=1000, depth=7, l2_leaf_reg=1)
    cat.fit(X_train, y_train)
    return cat
  

  